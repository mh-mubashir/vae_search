# VQVAE Metrics Documentation

## Overview

This document explains the metrics computed for VQVAE evaluation and why certain metrics (BCE and KL divergence) are handled differently compared to probabilistic VAE models like BetaVAE.

## Metrics Computed

### Reconstruction Metrics

The evaluation script computes the following metrics when evaluating VQVAE models:

1. **MSE** (Mean Squared Error)
   - Primary reconstruction loss used by VQVAE
   - Measures pixel-wise squared difference between original and reconstructed images
   - Lower is better (0 = perfect reconstruction)

2. **BCE** (Binary Cross-Entropy)
   - Computed for comparison purposes only
   - **Note**: VQVAE does NOT use BCE in its loss function (it's hardcoded to MSE)
   - Included to allow comparison with BetaVAE metrics

3. **VQ_Loss** (Vector Quantization Loss)
   - Replaces KL divergence in VQVAE
   - Measures how well the codebook represents encoder outputs
   - Consists of commitment loss + quantization loss
   - Lower indicates better codebook utilization

4. **Total_Loss**
   - Sum of reconstruction loss (MSE) and VQ loss
   - The overall training objective

5. **LPIPS** (Learned Perceptual Image Patch Similarity)
   - Perceptual similarity metric using VGG network
   - Lower is better (0 = identical perceptually)

6. **SSIM** (Structural Similarity Index)
   - Measures structural similarity between images
   - Higher is better (1 = identical structure)

7. **GMSD** (Gradient Magnitude Similarity Deviation)
   - Measures gradient-based similarity
   - Lower is better (0 = identical gradients)

### Generation Metrics

Same metrics as above, but computed between:
- `x_gen`: Images generated by sampling random codebook embeddings
- `x_gen_hat`: Reconstructions of those generated images

These measure:
- How well the model can generate realistic images from random codebook samples
- Model consistency (generated images should reconstruct well)

## Why BCE and KL Are Different in VQVAE

### Binary Cross-Entropy (BCE)

**In BetaVAE:**
- BetaVAE can use either MSE or BCE as reconstruction loss
- Controlled by `reconstruction_loss` parameter in config
- BCE is appropriate for binary data or when treating pixel values as probabilities

**In VQVAE:**
- **VQVAE only uses MSE** - it's hardcoded in the loss function
- No option to use BCE reconstruction loss
- The evaluation script computes BCE for comparison purposes, but VQVAE never uses it during training

**Code Reference:**
```python
# VQVAE loss function (from pythae/src/pythae/models/vq_vae/vq_vae_model.py)
def loss_function(self, recon_x, x, quantizer_output):
    recon_loss = F.mse_loss(  # Always MSE, no BCE option
        recon_x.reshape(x.shape[0], -1), 
        x.reshape(x.shape[0], -1), 
        reduction="none"
    ).sum(dim=-1)
    vq_loss = quantizer_output.loss
    return (recon_loss + vq_loss).mean(dim=0), recon_loss.mean(dim=0), vq_loss.mean(dim=0)
```

### Kullback-Leibler (KL) Divergence

**In BetaVAE (and other probabilistic VAEs):**
- BetaVAE is a **probabilistic model**
- Encoder outputs:
  - `mu`: Mean of the posterior distribution q(z|x)
  - `log_var`: Log variance of the posterior distribution
- Samples latent code: `z ~ N(mu, exp(log_var))` using reparameterization trick
- Computes KL divergence: `KL(q(z|x) || p(z))` where `p(z) = N(0, I)` is the prior
- KL measures how much the learned posterior deviates from the standard normal prior
- Used as regularization term in the loss: `Loss = Recon_Loss + β * KL`

**In VQVAE:**
- VQVAE is a **deterministic model** (not probabilistic)
- Encoder outputs:
  - Deterministic embeddings (no `mu` or `log_var`)
- No sampling: Uses vector quantization instead
  - Finds closest codebook entry for each embedding
  - Uses quantized embedding directly (no stochasticity)
- **No KL divergence** because:
  1. There's no probability distribution over latent codes
  2. No `mu` or `log_var` to compute KL from
  3. Latent codes are discrete (codebook indices), not continuous distributions
- **VQ Loss replaces KL** as the regularization term:
  - Commitment loss: Encourages encoder to output embeddings close to codebook entries
  - Quantization loss: Updates codebook entries to better represent encoder outputs
  - Loss: `Loss = Recon_Loss + VQ_Loss`

**Architectural Comparison:**

```
BetaVAE (Probabilistic):
Input → Encoder → [mu, log_var] → Sample z ~ N(mu, σ²) → Decoder → Output
                              ↓
                         KL(q(z|x)||p(z))

VQVAE (Deterministic):
Input → Encoder → Embeddings → Quantize (find closest codebook entry) → Decoder → Output
                              ↓
                         VQ Loss (commitment + quantization)
```

## Key Differences Summary

| Aspect | BetaVAE | VQVAE |
|--------|---------|-------|
| **Model Type** | Probabilistic | Deterministic |
| **Latent Space** | Continuous distribution | Discrete codebook |
| **Encoder Output** | `mu`, `log_var` | Deterministic embeddings |
| **Sampling** | Reparameterization trick | Vector quantization |
| **Reconstruction Loss** | MSE or BCE (configurable) | MSE only (hardcoded) |
| **Regularization** | KL divergence | VQ loss |
| **Latent Code** | `z ~ N(mu, σ²)` | Quantized codebook entry |

## Why This Matters

1. **Comparison with BetaVAE**: When comparing VQVAE and BetaVAE results:
   - Both compute MSE (though BetaVAE might use BCE)
   - VQ_Loss in VQVAE serves a similar role to KL in BetaVAE (regularization)
   - KL cannot be computed for VQVAE (not applicable)

2. **Model Selection**:
   - Use BetaVAE if you need probabilistic latent space and sampling
   - Use VQVAE if you need discrete latent representations and codebook learning

3. **Evaluation**:
   - The evaluation script computes BCE for VQVAE for comparison purposes
   - KL divergence is not computed (and cannot be) for VQVAE
   - VQ_Loss is the appropriate regularization metric for VQVAE

## References

- VQVAE Paper: [Neural Discrete Representation Learning](https://arxiv.org/abs/1711.00937)
- BetaVAE Paper: [β-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework](https://openreview.net/forum?id=Sy2fzU9gl)
- PyTorch-VAE Implementation: [pythae library](https://github.com/clementchadebec/benchmark_VAE)


