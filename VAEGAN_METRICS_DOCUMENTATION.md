# VAEGAN Metrics Documentation

## Overview

This document explains the metrics computed for VAEGAN (Variational Autoencoder Generative Adversarial Network) evaluation and how they differ from other VAE models like BetaVAE and VQVAE.

## Metrics Computed

### Reconstruction Metrics

The evaluation script computes the following metrics when evaluating VAEGAN models:

1. **MSE** (Mean Squared Error)
   - Pixel-wise reconstruction loss
   - Computed for comparison purposes (VAEGAN uses feature-space MSE)
   - Lower is better (0 = perfect reconstruction)

2. **BCE** (Binary Cross-Entropy)
   - Computed for comparison purposes
   - **Note**: VAEGAN uses feature-space MSE, not pixel-wise BCE or MSE
   - Included to allow comparison with BetaVAE metrics

3. **Recon_Loss** (Reconstruction Loss)
   - **VAEGAN's actual reconstruction loss**
   - MSE computed in discriminator feature space (not pixel space)
   - Uses intermediate discriminator layer features for perceptual reconstruction
   - Lower is better

4. **KL** (Kullback-Leibler Divergence)
   - Computed directly from encoder outputs (`mu`, `log_var`)
   - Measures divergence between posterior q(z|x) and prior p(z) = N(0, I)
   - Same as in BetaVAE (VAEGAN is probabilistic)
   - Lower indicates better regularization

5. **ELBO** (Evidence Lower Bound)
   - Computed as `ELBO = MSE + KL` for comparison with BetaVAE
   - **Note**: VAEGAN's actual training uses feature-space recon_loss, not pixel MSE
   - Included for consistency with BetaVAE evaluation

6. **Encoder_Loss**
   - Encoder component: `Encoder_Loss = KL + Recon_Loss`
   - Combines regularization (KL) and reconstruction (feature-space MSE)

7. **Decoder_Loss**
   - Decoder component with adversarial component
   - Formula: `Decoder_Loss = (1 - α) * Recon_Loss - α * Discriminator_Loss`
   - Where α is the adversarial loss scale
   - Balances reconstruction quality and adversarial training

8. **Discriminator_Loss**
   - Adversarial loss from discriminator
   - Measures discriminator's ability to distinguish real vs generated images
   - Lower indicates better generation quality (discriminator is fooled)

9. **Total_Loss**
   - Sum of encoder_loss + decoder_loss + discriminator_loss
   - The overall training objective

10. **LPIPS** (Learned Perceptual Image Patch Similarity)
    - Perceptual similarity metric using VGG network
    - Lower is better (0 = identical perceptually)

11. **SSIM** (Structural Similarity Index)
    - Measures structural similarity between images
    - Higher is better (1 = identical structure)

12. **GMSD** (Gradient Magnitude Similarity Deviation)
    - Measures gradient-based similarity
    - Lower is better (0 = identical gradients)

### Generation Metrics

Same metrics as above, but computed between:
- `x_gen`: Images generated by sampling `z ~ N(0, I)` from prior
- `x_gen_hat`: Reconstructions of those generated images

These measure:
- How well the model can generate realistic images from random latent samples
- Model consistency (generated images should reconstruct well)

## Key Differences: VAEGAN vs BetaVAE vs VQVAE

### Architecture Comparison

**BetaVAE (Probabilistic VAE):**
```
Input → Encoder → [mu, log_var] → Sample z ~ N(mu, σ²) → Decoder → Output
                              ↓
                         KL(q(z|x)||p(z))
Loss = Recon_Loss + β * KL
```

**VQVAE (Deterministic with Quantization):**
```
Input → Encoder → Embeddings → Quantize (codebook) → Decoder → Output
                              ↓
                         VQ Loss (commitment + quantization)
Loss = Recon_Loss + VQ_Loss
```

**VAEGAN (Probabilistic VAE + GAN):**
```
Input → Encoder → [mu, log_var] → Sample z ~ N(mu, σ²) → Decoder → Output
                              ↓                              ↓
                         KL(q(z|x)||p(z))              Discriminator
                                                              ↓
                                                    Feature-space MSE
Loss = Encoder_Loss + Decoder_Loss + Discriminator_Loss
where:
  Encoder_Loss = KL + Recon_Loss (feature-space)
  Decoder_Loss = (1-α) * Recon_Loss - α * Discriminator_Loss
  Discriminator_Loss = BCE(real) + BCE(fake)
```

### Reconstruction Loss

**BetaVAE:**
- Uses pixel-wise MSE or BCE (configurable)
- Direct pixel-to-pixel comparison

**VQVAE:**
- Uses pixel-wise MSE only (hardcoded)
- Direct pixel-to-pixel comparison

**VAEGAN:**
- Uses **feature-space MSE** (not pixel-wise)
- Computes MSE between discriminator feature maps:
  - Real image → Discriminator → Feature maps
  - Reconstructed image → Discriminator → Feature maps
  - MSE computed in feature space (more perceptual)
- This encourages perceptually better reconstructions

### Regularization

**BetaVAE:**
- KL divergence: `KL(q(z|x) || p(z))`
- Regularizes posterior to match prior N(0, I)

**VQVAE:**
- VQ Loss: Commitment loss + Quantization loss
- No probabilistic regularization

**VAEGAN:**
- KL divergence: Same as BetaVAE
- **Plus** adversarial regularization via discriminator
- Discriminator encourages realistic generations

### Generation Process

**BetaVAE:**
- Sample `z ~ N(0, I)` from prior
- Decode: `x = Decoder(z)`

**VQVAE:**
- Sample random codebook embeddings
- Decode: `x = Decoder(quantized_embeddings)`

**VAEGAN:**
- Sample `z ~ N(0, I)` from prior (same as BetaVAE)
- Decode: `x = Decoder(z)`
- Discriminator provides additional training signal

## Why Feature-Space Reconstruction Loss?

VAEGAN uses feature-space MSE instead of pixel-wise MSE because:

1. **Perceptual Quality**: Discriminator features capture high-level perceptual information
2. **Adversarial Training**: Aligns with GAN training paradigm
3. **Better Reconstructions**: Encourages perceptually similar images, not just pixel-accurate ones
4. **Stability**: Feature-space losses are often more stable than pixel losses

**Code Reference:**
```python
# VAEGAN loss function (from pythae/src/pythae/models/vae_gan/vae_gan_model.py)
# Feature maps of true data
true_discr_layer = self.discriminator(
    x, output_layer_levels=[self.reconstruction_layer]
)[f"embedding_layer_{self.reconstruction_layer}"]

# Feature maps of recon data
recon_discr_layer = self.discriminator(
    recon_x, output_layer_levels=[self.reconstruction_layer]
)[f"embedding_layer_{self.reconstruction_layer}"]

# MSE in feature space
recon_loss = 0.5 * F.mse_loss(
    true_discr_layer.reshape(N, -1),
    recon_discr_layer.reshape(N, -1),
    reduction="none",
).sum(dim=-1)
```

## Key Differences Summary

| Aspect | BetaVAE | VQVAE | VAEGAN |
|--------|---------|-------|--------|
| **Model Type** | Probabilistic | Deterministic | Probabilistic + Adversarial |
| **Latent Space** | Continuous distribution | Discrete codebook | Continuous distribution |
| **Encoder Output** | `mu`, `log_var` | Deterministic embeddings | `mu`, `log_var` |
| **Sampling** | Reparameterization trick | Vector quantization | Reparameterization trick |
| **Reconstruction Loss** | Pixel MSE/BCE | Pixel MSE | **Feature-space MSE** |
| **Regularization** | KL divergence | VQ loss | KL + Adversarial |
| **Discriminator** | No | No | Yes |
| **Generation** | `z ~ N(0, I)` | Codebook sampling | `z ~ N(0, I)` |
| **Training** | Single optimizer | Single optimizer | **Coupled optimizers** |

## Understanding the Loss Components

### Encoder Loss
```
Encoder_Loss = KL + Recon_Loss
```
- **KL**: Regularizes latent distribution
- **Recon_Loss**: Feature-space reconstruction quality
- Encoder learns to encode inputs into well-regularized latent codes

### Decoder Loss
```
Decoder_Loss = (1 - α) * Recon_Loss - α * Discriminator_Loss
```
- **Recon_Loss**: Ensures reconstructions match inputs (feature space)
- **Discriminator_Loss**: Encourages realistic generations
- **α (adversarial_loss_scale)**: Balances reconstruction vs generation quality
- Decoder learns to generate realistic images

### Discriminator Loss
```
Discriminator_Loss = BCE(real_images, 1) + BCE(generated_images, 0)
```
- Trains discriminator to distinguish real from generated
- Lower loss = better generation quality (discriminator is fooled)

## Why This Matters

1. **Comparison with BetaVAE**:
   - Both have KL divergence (probabilistic models)
   - VAEGAN uses feature-space recon_loss vs pixel MSE/BCE
   - VAEGAN has additional discriminator metrics

2. **Comparison with VQVAE**:
   - VAEGAN is probabilistic (has KL), VQVAE is deterministic
   - VAEGAN uses adversarial training, VQVAE uses quantization
   - Different regularization approaches

3. **Model Selection**:
   - Use **BetaVAE** for simple probabilistic VAE with pixel losses
   - Use **VQVAE** for discrete latent representations
   - Use **VAEGAN** for high-quality generation with adversarial training

4. **Evaluation**:
   - **Recon_Loss** is VAEGAN's actual reconstruction metric (feature-space)
   - **MSE** is computed for comparison with other models
   - **Discriminator_Loss** indicates generation quality
   - **KL** measures latent space regularization

## Training Dynamics

VAEGAN uses **coupled optimizers** with conditional updates:

- **Encoder**: Always updated
- **Decoder**: Updated unless discriminator is too weak
- **Discriminator**: Updated unless it's too strong

This ensures balanced training between generator and discriminator.

## References

- VAEGAN Paper: [Autoencoding beyond pixels using a learned similarity metric](https://arxiv.org/abs/1512.09300)
- BetaVAE Paper: [β-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework](https://openreview.net/forum?id=Sy2fzU9gl)
- VQVAE Paper: [Neural Discrete Representation Learning](https://arxiv.org/abs/1711.00937)
- PyTorch-VAE Implementation: [pythae library](https://github.com/clementchadebec/benchmark_VAE)

